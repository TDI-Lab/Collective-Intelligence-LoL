{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just a check in the code that corresponding timeline and match data files were downloaded for same matches and that there were no mismatches!\n",
    "\n",
    "matchDataDirectory = '../matchData'\n",
    "matchTimelineDirectory = '../matchTimeline'\n",
    "\n",
    "mdfiles = os.listdir(matchDataDirectory)\n",
    "mlfiles = os.listdir(matchTimelineDirectory)\n",
    "\n",
    "mdjsonfiles = [file for file in mdfiles if file.endswith('.json')]\n",
    "mljsonfiles = [file for file in mlfiles if file.endswith('.json')]\n",
    "\n",
    "print(len(mdjsonfiles))\n",
    "print(len(mljsonfiles))\n",
    "\n",
    "mdfilesset = set(mdjsonfiles)\n",
    "mlfilesset = set(mljsonfiles)\n",
    "\n",
    "if mdfilesset == mlfilesset:\n",
    "    print(\"Corresponding match data and timeline files exists\")\n",
    "else:\n",
    "    print(\"Missing match data or time line files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There were some match files for which the API returned match data with content as null. So this is for identifying such files. \n",
    "## Correspondingly time lines should also be deleted! Deletion was done manually from terminal\n",
    "## Cross-check inidividually for matchTimeline folder as well\n",
    "\n",
    "# Define the directory and the file extension/type\n",
    "directory = '../matchTimeline'\n",
    "file_type = '.json'  # Change this to your desired file type\n",
    "\n",
    "# Initialize an empty list to store the valid files\n",
    "null_files = []\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "\n",
    "    # Check if the file is of the desired type\n",
    "    if file_name.endswith(file_type):\n",
    "        # Open and read the content of the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read().strip()  # Remove any surrounding whitespace\n",
    "\n",
    "        # Check if the content is not 'null'\n",
    "        if content == \"null\":\n",
    "            # Add the file to the valid_files list\n",
    "            null_files.append(file_name)\n",
    "\n",
    "# Output the list of null files\n",
    "print(\"Files with null content:\")\n",
    "print(null_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the two files containing match data and summoner's rank\n",
    "matchDF = pd.read_csv('../aggregateMatchAndTimeline.csv')\n",
    "print(matchDF.columns)\n",
    "\n",
    "summonerRankDF = pd.read_csv('../summonerRanks.csv')\n",
    "print(summonerRankDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchWithSummonerRankDF = matchDF.merge(summonerRankDF, on='summonerId', how='right')\n",
    "matchWithSummonerRankDF.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchPlayerCounts = matchWithSummonerRankDF.groupby(['gameId'])['summonerId'].count().reset_index()\n",
    "matchPlayerCounts.rename(columns={'summonerId': 'playerCounts'}, inplace=True)\n",
    "print(matchPlayerCounts.head())\n",
    "\n",
    "\n",
    "matchWithSummonerRankDF = matchWithSummonerRankDF.merge(matchPlayerCounts, on='gameId', how='inner')\n",
    "\n",
    "valid_matches = matchWithSummonerRankDF[matchWithSummonerRankDF['playerCounts'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of numerical rank of each individual player and the average rank in the play\n",
    "\n",
    "# join the tier and rank to yield a combined value\n",
    "valid_matches['tierRank'] = valid_matches['tier'] + '-' + valid_matches['rank']\n",
    "\n",
    "rankedDict = {\n",
    "    'NA':0,\n",
    "    'IRON-IV':1,\n",
    "    'IRON-III':2,\n",
    "    'IRON-II':3,\n",
    "    'IRON-I':4,\n",
    "    'BRONZE-IV':5,\n",
    "    'BRONZE-III':6,\n",
    "    'BRONZE-II':7,\n",
    "    'BRONZE-I':8,\n",
    "    'SILVER-IV':9,\n",
    "    'SILVER-III':10,\n",
    "    'SILVER-II':11,\n",
    "    'SILVER-I':12,\n",
    "    'GOLD-IV':13,\n",
    "    'GOLD-III':14,\n",
    "    'GOLD-II':15,\n",
    "    'GOLD-I':16,\n",
    "    'PLATINUM-IV':17,\n",
    "    'PLATINUM-III':18,\n",
    "    'PLATINUM-II':19,\n",
    "    'PLATINUM-I':20,\n",
    "    'EMERALD-IV':21,\n",
    "    'EMERALD-III':22,\n",
    "    'EMERALD-II':23,\n",
    "    'EMERALD-I':24,\n",
    "    'DIAMOND-IV':25,\n",
    "    'DIAMOND-III':26,\n",
    "    'DIAMOND-II':27,\n",
    "    'DIAMOND-I':28,\n",
    "    'MASTER-I':29,\n",
    "    'GRANDMASTER-I':30,\n",
    "    'CHALLENGER-I':31\n",
    "}\n",
    "\n",
    "def calculateNumericalRank(row):\n",
    "    return rankedDict[row['tierRank']]\n",
    "\n",
    "valid_matches['numericalRank'] = valid_matches.apply(calculateNumericalRank, axis=1)\n",
    "\n",
    "averageRankDF = valid_matches.groupby(['gameId'])['numericalRank'].mean().reset_index()\n",
    "averageRankDF.rename(columns={'numericalRank': 'avgrank'}, inplace=True)\n",
    "\n",
    "valid_matches = valid_matches.merge(averageRankDF, on='gameId', how='inner')\n",
    "\n",
    "print(valid_matches.head())\n",
    "\n",
    "valid_matches.sort_values(by=['gameId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "valid_matches.to_csv('../finalDataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_matches[['participantId','participantsAssisted','towerKillsAssisted','monsterKillsAssisted','participantsAssistedWithPressure']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(valid_matches['teamId'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is named 'df' with columns 'to', 'from', and 'unit'\n",
    "# If not, replace 'df' with your actual dataframe name\n",
    "df = pd.read_csv('../data_building/team/assists.csv') ## weight file\n",
    "df = df[df['gid'] == 'EUW1_6895619260']\n",
    "\n",
    "df = df[df['weight'] != 0]\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges to the graph\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(row['frm'], row['to_player'], weight=row['weight'])\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a layout for the nodes\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Draw the nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=800)\n",
    "\n",
    "# Draw the edges\n",
    "edges = nx.draw_networkx_edges(\n",
    "    G,\n",
    "    pos,\n",
    "    width=2,\n",
    "    edge_color='gray',\n",
    "    arrowsize=12\n",
    ")\n",
    "\n",
    "# Add edge labels (weights)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "# Add node labels\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "# Remove axis\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-leeds-codethesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
