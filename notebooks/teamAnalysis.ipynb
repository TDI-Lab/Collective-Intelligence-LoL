{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamAllMetrics = pd.read_csv('./teamAllMetrics.csv')\n",
    "teamAllMetrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming an analysis to see if higher gold is always a factor that results in winning games. Need to normalize on the gold earned for each team on the basis of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchTotalMetrics = teamAllMetrics.groupby(['gameId'])[['totalTeamGold', 'totalTeamKills', 'totalTeamChampExperience', 'totalTeamMinionsKilled']].sum().reset_index()\n",
    "matchTotalMetrics.rename(columns={'totalTeamGold': 'matchTotalGold', 'totalTeamKills': 'matchTotalKills', 'totalTeamChampExperience': 'matchTotalChampExperience', 'totalTeamMinionsKilled': 'matchTotalMinionsKilled'}, inplace=True)\n",
    "\n",
    "teamAllMetrics = teamAllMetrics.merge(matchTotalMetrics, on=['gameId'], how='inner')\n",
    "print(teamAllMetrics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamAllMetrics['teamMatchGoldShare'] = teamAllMetrics['totalTeamGold'] / teamAllMetrics['matchTotalGold']\n",
    "teamAllMetrics['teamMatchExpShare'] = teamAllMetrics['totalTeamChampExperience'] / teamAllMetrics['matchTotalChampExperience']\n",
    "teamAllMetrics['teamMatchKillsShare'] = teamAllMetrics['totalTeamKills'] / teamAllMetrics['matchTotalKills']\n",
    "teamAllMetrics['teamMatchMinionsShare'] = teamAllMetrics['totalTeamMinionsKilled'] / teamAllMetrics['matchTotalMinionsKilled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamAllMetrics[['gameId', 'teamId', 'teamMatchGoldShare', 'win']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in 95 games, there are cases when lower gold team wins the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamAllMetrics[(teamAllMetrics['teamMatchGoldShare'] > 0.5) & (teamAllMetrics['win'] == 0)].describe()\n",
    "\n",
    "lesserGoldWinningTeams = teamAllMetrics[(teamAllMetrics['teamMatchGoldShare'] > 0.5) & (teamAllMetrics['win'] == 0)]\n",
    "lesserGoldWinningTeams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare match outcomes for both the teams in for lesser winning team winning matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesserGoldWinningTeamsGameIds = lesserGoldWinningTeams['gameId']\n",
    "lesserGoldWinningTeamMatches = teamAllMetrics[teamAllMetrics['gameId'].isin(lesserGoldWinningTeamsGameIds)]\n",
    "print(lesserGoldWinningTeamMatches.head(10))\n",
    "\n",
    "subsetDF = lesserGoldWinningTeamMatches[['gameId', 'teamId', 'win', 'teamMatchExpShare', 'teamMatchGoldShare', 'teamMatchKillsShare', 'resistance', 'teamIndegreeCentrality', 'teamOutdegreeCentrality']]\n",
    "subsetDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Multi-Collinearity Between Variables - Using Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamAllMetrics['totalTeamKillsPerMin'] = teamAllMetrics['totalTeamKills'] / teamAllMetrics['gameDuration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# using original set of metrics before, we had share of the match\n",
    "teamDF = teamAllMetrics[[   'gameId', 'teamId', 'win', 'teamAverageRank', 'totalTeamKillsPerMin', 'totalTeamAllAssistsPerMin', 'totalTeamEpicMonsterKills', 'totalTeamTurretKills',\n",
    "                            'teamMatchGoldShare', 'goldPerMin', 'totalTeamChampExpPerMin', 'teamMatchMinionsShare', 'totalTeamVisionPerMin',\n",
    "                            'resistance', 'teamIndegreeCentrality',\n",
    "                            'teamOutdegreeCentrality', 'teamWeightCentralization'\n",
    "                        ]]\n",
    "\n",
    "# subset of features from teamDF that will be used for checking collinearity using VIF\n",
    "y, X = dmatrices('win~teamAverageRank+totalTeamKillsPerMin+totalTeamAllAssistsPerMin+totalTeamEpicMonsterKills+totalTeamTurretKills+teamMatchGoldShare+goldPerMin+totalTeamChampExpPerMin+teamMatchMinionsShare+totalTeamVisionPerMin+resistance+teamIndegreeCentrality+teamOutdegreeCentrality', data=teamDF, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = pd.DataFrame()\n",
    "vif_df['variable'] = X.columns \n",
    "vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for multi-collinearity using pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Correlation matrix\n",
    "teamDF_features = teamDF[['teamAverageRank', 'totalTeamKillsPerMin', 'totalTeamAllAssistsPerMin', 'totalTeamEpicMonsterKills', 'totalTeamTurretKills',\n",
    "                        'teamMatchGoldShare', 'goldPerMin', 'totalTeamChampExpPerMin', 'teamMatchMinionsShare', 'totalTeamVisionPerMin',\n",
    "                        'resistance', 'teamIndegreeCentrality',\n",
    "                        'teamOutdegreeCentrality', 'teamWeightCentralization'\n",
    "                    ]]\n",
    "\n",
    "correlation_matrix = teamDF_features.corr()\n",
    "\n",
    "# Function to calculate p-values\n",
    "def correlation_test(x, y):\n",
    "    return stats.pearsonr(x, y)\n",
    "\n",
    "# Calculating p-values\n",
    "p_values = pd.DataFrame(index=teamDF_features.columns, columns=teamDF_features.columns)\n",
    "\n",
    "for col1 in teamDF_features.columns:\n",
    "    for col2 in teamDF_features.columns:\n",
    "        if col1 == col2:\n",
    "            p_values[col1][col2] = np.nan  # NaN for the diagonal\n",
    "        else:\n",
    "            corr_test = correlation_test(teamDF_features[col1], teamDF_features[col2])\n",
    "            p_values[col1][col2] = corr_test[1]  # Extract the p-value\n",
    "\n",
    "# print(p_values)\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "\n",
    "teamDF_features.drop(['goldPerMin', 'teamMatchGoldShare', 'teamWeightCentralization'], axis=1, inplace=True)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and prediction using in-game + graph metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting games into train-test split\n",
    "game_ids = teamDF['gameId'].unique()\n",
    "train_games, test_games = train_test_split(game_ids, test_size=0.2, random_state=245)\n",
    "\n",
    "train_df = teamDF[teamDF['gameId'].isin(train_games)]\n",
    "test_df = teamDF[teamDF['gameId'].isin(test_games)]\n",
    "\n",
    "X_train = train_df[['teamAverageRank', 'totalTeamKillsPerMin', 'totalTeamAllAssistsPerMin', 'totalTeamEpicMonsterKills', 'totalTeamTurretKills', 'totalTeamChampExpPerMin', 'teamMatchMinionsShare', 'totalTeamVisionPerMin', 'resistance', 'teamIndegreeCentrality', 'teamOutdegreeCentrality']]\n",
    "y_train = train_df['win']\n",
    "\n",
    "X_test = test_df[['teamAverageRank', 'totalTeamKillsPerMin', 'totalTeamAllAssistsPerMin', 'totalTeamEpicMonsterKills', 'totalTeamTurretKills', 'totalTeamChampExpPerMin', 'teamMatchMinionsShare', 'totalTeamVisionPerMin', 'resistance', 'teamIndegreeCentrality', 'teamOutdegreeCentrality']]\n",
    "y_test = test_df['win']\n",
    "\n",
    "features_names = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for results with and without scaling\n",
    "# Scaling data features to avoid model skewing; performing scaling may not be necessary, because we have transformed the data!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=400)\n",
    "model.fit(X_train, y_train) # use X_train_scaled to fit instead\n",
    "\n",
    "features_names = X_train.columns\n",
    "coefficients = model.coef_\n",
    "importances = coefficients[0]\n",
    "blr_feature_importances = dict(zip(features_names, importances))\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-values using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X_train) # adding a constant intercept term\n",
    "logit_model = sm.Logit(y_train, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(blr_feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtmodel = DecisionTreeClassifier()\n",
    "dtmodel.fit(X_train, y_train)\n",
    "dt_importances = dtmodel.feature_importances_\n",
    "feature_importances = dict(zip(features_names, dt_importances))\n",
    "\n",
    "\n",
    "dt_y_pred = dtmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, dt_y_pred)\n",
    "\n",
    "print(feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfmodel = RandomForestClassifier(n_estimators=16, max_depth=8, bootstrap=True)\n",
    "rfmodel.fit(X_train, y_train)\n",
    "rf_importances = rfmodel.feature_importances_ \n",
    "rf_feature_importances = dict(zip(features_names, rf_importances))\n",
    "rf_y_pred = rfmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, rf_y_pred)\n",
    "\n",
    "print(rf_feature_importances)\n",
    "print('RF Accuracy:', accuracy)\n",
    "print('RF Confusion Matrix:\\n', conf_matrix)\n",
    "print('RF Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    n_estimators = 100,\n",
    "    eval_metric='logloss',\n",
    "    learning_rate = 0.05\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_importances = xgb_model.feature_importances_ \n",
    "xgb_feature_importances = dict(zip(features_names, xgb_importances))\n",
    "\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(xgb_feature_importances)\n",
    "print('XGB Accuracy:', accuracy)\n",
    "print('XGB Confusion Matrix:\\n', conf_matrix)\n",
    "print('XGB Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and predicition using graph metrics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting games into train-test split\n",
    "game_ids = teamDF['gameId'].unique()\n",
    "train_games, test_games = train_test_split(game_ids, test_size=0.2, random_state=25)\n",
    "\n",
    "train_df = teamDF[teamDF['gameId'].isin(train_games)]\n",
    "test_df = teamDF[teamDF['gameId'].isin(test_games)]\n",
    "\n",
    "X_train = train_df[['resistance', 'teamIndegreeCentrality', 'teamOutdegreeCentrality']]\n",
    "y_train = train_df['win']\n",
    "\n",
    "X_test = test_df[['resistance', 'teamIndegreeCentrality', 'teamOutdegreeCentrality']]\n",
    "y_test = test_df['win']\n",
    "\n",
    "features_names = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=400)\n",
    "model.fit(X_train, y_train) # use X_train_scaled to fit instead\n",
    "\n",
    "features_names = X_train.columns\n",
    "coefficients = model.coef_\n",
    "importances = coefficients[0]\n",
    "blr_feature_importances = dict(zip(features_names, importances))\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-values using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X_train) # adding a constant intercept term\n",
    "logit_model = sm.Logit(y_train, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(blr_feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtmodel = DecisionTreeClassifier()\n",
    "dtmodel.fit(X_train, y_train)\n",
    "dt_importances = dtmodel.feature_importances_\n",
    "feature_importances = dict(zip(features_names, dt_importances))\n",
    "\n",
    "\n",
    "dt_y_pred = dtmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, dt_y_pred)\n",
    "\n",
    "print(feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfmodel = RandomForestClassifier(n_estimators=16, max_depth=8, bootstrap=True)\n",
    "rfmodel.fit(X_train, y_train)\n",
    "rf_importances = rfmodel.feature_importances_ \n",
    "rf_feature_importances = dict(zip(features_names, rf_importances))\n",
    "rf_y_pred = rfmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, rf_y_pred)\n",
    "\n",
    "print(rf_feature_importances)\n",
    "print('RF Accuracy:', accuracy)\n",
    "print('RF Confusion Matrix:\\n', conf_matrix)\n",
    "print('RF Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    n_estimators = 100,\n",
    "    eval_metric='logloss',\n",
    "    learning_rate = 0.05\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_importances = xgb_model.feature_importances_ \n",
    "xgb_feature_importances = dict(zip(features_names, xgb_importances))\n",
    "\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(xgb_feature_importances)\n",
    "print('XGB Accuracy:', accuracy)\n",
    "print('XGB Confusion Matrix:\\n', conf_matrix)\n",
    "print('XGB Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Prediction with In-Game Metrics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting games into train-test split\n",
    "game_ids = teamDF['gameId'].unique()\n",
    "train_games, test_games = train_test_split(game_ids, test_size=0.2, random_state=245)\n",
    "\n",
    "train_df = teamDF[teamDF['gameId'].isin(train_games)]\n",
    "test_df = teamDF[teamDF['gameId'].isin(test_games)]\n",
    "\n",
    "X_train = train_df[['teamAverageRank', 'totalTeamKillsPerMin', 'totalTeamAllAssistsPerMin', 'totalTeamEpicMonsterKills', 'totalTeamTurretKills', 'totalTeamChampExpPerMin', 'teamMatchMinionsShare', 'totalTeamVisionPerMin']]\n",
    "y_train = train_df['win']\n",
    "\n",
    "X_test = test_df[['teamAverageRank', 'totalTeamKillsPerMin', 'totalTeamAllAssistsPerMin', 'totalTeamEpicMonsterKills', 'totalTeamTurretKills', 'totalTeamChampExpPerMin', 'teamMatchMinionsShare', 'totalTeamVisionPerMin']]\n",
    "y_test = test_df['win']\n",
    "\n",
    "features_names = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=400)\n",
    "model.fit(X_train, y_train) # use X_train_scaled to fit instead\n",
    "\n",
    "features_names = X_train.columns\n",
    "coefficients = model.coef_\n",
    "importances = coefficients[0]\n",
    "blr_feature_importances = dict(zip(features_names, importances))\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-values using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X_train) # adding a constant intercept term\n",
    "logit_model = sm.Logit(y_train, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(blr_feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtmodel = DecisionTreeClassifier()\n",
    "dtmodel.fit(X_train, y_train)\n",
    "dt_importances = dtmodel.feature_importances_\n",
    "feature_importances = dict(zip(features_names, dt_importances))\n",
    "\n",
    "\n",
    "dt_y_pred = dtmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, dt_y_pred)\n",
    "\n",
    "print(feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = RandomForestClassifier(n_estimators=16, max_depth=8, bootstrap=True)\n",
    "rfmodel.fit(X_train, y_train)\n",
    "rf_importances = rfmodel.feature_importances_ \n",
    "rf_feature_importances = dict(zip(features_names, rf_importances))\n",
    "rf_y_pred = rfmodel.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, rf_y_pred)\n",
    "\n",
    "print(rf_feature_importances)\n",
    "print('RF Accuracy:', accuracy)\n",
    "print('RF Confusion Matrix:\\n', conf_matrix)\n",
    "print('RF Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    n_estimators = 100,\n",
    "    eval_metric='logloss',\n",
    "    learning_rate = 0.05\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_importances = xgb_model.feature_importances_ \n",
    "xgb_feature_importances = dict(zip(features_names, xgb_importances))\n",
    "\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(xgb_feature_importances)\n",
    "print('XGB Accuracy:', accuracy)\n",
    "print('XGB Confusion Matrix:\\n', conf_matrix)\n",
    "print('XGB Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "teamDF_features_scaled = scaler.fit_transform(teamDF_features)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(teamDF_features_scaled)\n",
    "\n",
    "\n",
    "# Plot the Scree Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(1, len(pca.explained_variance_)+1), pca.explained_variance_, marker='o')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Analysis; scree plot suggests number of latent components to be 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Factor Analysis\n",
    "fa = FactorAnalysis(n_components=3)  # Adjust n_components based on Scree Plot\n",
    "fa.fit(teamDF_features_scaled)\n",
    "factor_loadings = fa.components_.T\n",
    "\n",
    "# factor loadings for each feature column\n",
    "loadings_df = pd.DataFrame(factor_loadings, index=teamDF_features.columns, columns=[f'Factor{i+1}' for i in range(factor_loadings.shape[1])])\n",
    "\n",
    "# compute factor scores\n",
    "factor_scores = fa.transform(teamDF_features_scaled)\n",
    "factor_scores_df = pd.DataFrame(factor_scores, columns=[f'Factor{i+1}' for i in range(factor_scores.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print factor loadings\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BLR on computed factor scores\n",
    "y = teamDF['win']\n",
    "X = factor_scores_df\n",
    "\n",
    "# splitting games into train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=400)\n",
    "model.fit(X_train, y_train) # use X_train_scaled to fit instead\n",
    "\n",
    "features_names = X_train.columns\n",
    "coefficients = model.coef_\n",
    "importances = coefficients[0]\n",
    "blr_feature_importances = dict(zip(features_names, importances))\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-values using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X_train) # adding a constant intercept term\n",
    "logit_model = sm.Logit(y_train, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(blr_feature_importances)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Collective-Intelligence-LoL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
